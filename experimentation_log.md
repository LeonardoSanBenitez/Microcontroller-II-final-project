# Experiment log
## Initial
```
on ep. 0, mean reward is nan (epsilon is 0.900)
on ep. 3000, mean reward is -174.30 (epsilon is 0.494)
on ep. 6000, mean reward is -113.65 (epsilon is 0.271)
on ep. 9000, mean reward is -84.33 (epsilon is 0.149)
on ep. 12000, mean reward is -70.38 (epsilon is 0.082)
on ep. 15000, mean reward is -58.04 (epsilon is 0.045)
on ep. 18000, mean reward is -48.46 (epsilon is 0.025)
on ep. 21000, mean reward is -42.77 (epsilon is 0.013)
on ep. 24000, mean reward is -36.74 (epsilon is 0.007)
```

## New actions
```
on ep. 0, mean reward is nan (epsilon is 0.900)
on ep. 3000, mean reward is -179.33 (epsilon is 0.494)
on ep. 6000, mean reward is -130.48 (epsilon is 0.271)
on ep. 9000, mean reward is -88.94 (epsilon is 0.149)
on ep. 12000, mean reward is -63.96 (epsilon is 0.082)
on ep. 15000, mean reward is -47.53 (epsilon is 0.045)
on ep. 18000, mean reward is -38.93 (epsilon is 0.025)
on ep. 21000, mean reward is -33.63 (epsilon is 0.013)
on ep. 24000, mean reward is -24.95 (epsilon is 0.007)
```

## Size 3
```
on ep. 0, mean reward is nan (epsilon is 0.900)
on ep. 3000, mean reward is -90.15 (epsilon is 0.494)
on ep. 6000, mean reward is -43.20 (epsilon is 0.271)
on ep. 9000, mean reward is -25.77 (epsilon is 0.149)
on ep. 12000, mean reward is -19.61 (epsilon is 0.082)
on ep. 15000, mean reward is -13.44 (epsilon is 0.045)
on ep. 18000, mean reward is -7.70 (epsilon is 0.025)
on ep. 21000, mean reward is -5.25 (epsilon is 0.013)
on ep. 24000, mean reward is -1.26 (epsilon is 0.007)
```

## Size 3, no special final reward, longer training
```
on ep. 3000, mean reward is -91.97 (epsilon is 0.494)
on ep. 6000, mean reward is -51.47 (epsilon is 0.271)
on ep. 9000, mean reward is -31.59 (epsilon is 0.149)
on ep. 12000, mean reward is -20.12 (epsilon is 0.082)
on ep. 15000, mean reward is -15.97 (epsilon is 0.045)
on ep. 18000, mean reward is -4.86 (epsilon is 0.025)
on ep. 21000, mean reward is -5.68 (epsilon is 0.013)
on ep. 24000, mean reward is -1.78 (epsilon is 0.007)
on ep. 27000, mean reward is -4.25 (epsilon is 0.004)
```

## SIZE = 3, GRID_SIZE = 10, 30k episodes, reward  Near food
```
on ep. 0, mean reward is nan (epsilon is 0.900)
on ep. 3000, mean reward is 123.05 (epsilon is 0.775)
on ep. 6000, mean reward is 194.16 (epsilon is 0.667)
on ep. 9000, mean reward is 223.99 (epsilon is 0.574)
on ep. 12000, mean reward is 237.12 (epsilon is 0.494)
on ep. 15000, mean reward is 252.09 (epsilon is 0.425)
on ep. 18000, mean reward is 260.69 (epsilon is 0.366)
on ep. 21000, mean reward is 266.70 (epsilon is 0.315)
on ep. 24000, mean reward is 269.16 (epsilon is 0.271)
on ep. 27000, mean reward is 276.57 (epsilon is 0.233)
```

## Same as beore but correcting new_obs
```
on ep. 0, mean reward is nan (epsilon is 0.900)
on ep. 3000, mean reward is 144.39 (epsilon is 0.775)
on ep. 6000, mean reward is 208.71 (epsilon is 0.667)
on ep. 9000, mean reward is 236.41 (epsilon is 0.574)
on ep. 12000, mean reward is 252.80 (epsilon is 0.494)
on ep. 15000, mean reward is 264.07 (epsilon is 0.425)
on ep. 18000, mean reward is 269.98 (epsilon is 0.366)
on ep. 21000, mean reward is 274.00 (epsilon is 0.315)
on ep. 24000, mean reward is 278.05 (epsilon is 0.271)
on ep. 27000, mean reward is 279.49 (epsilon is 0.233)
```